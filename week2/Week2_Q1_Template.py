# -*- coding: utf-8 -*-
"""Assignment2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x6cDS5cpw-0-OifM4z--ULQPIA9hVX3P
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline

#data = np.loadtxt('prob1data.txt', delimiter=',')

def plot_ex_data(x,y,xlabel,ylabel):
  plt.figure()
  plt.plot(x,y,'o')
  plt.xlabel(xlabel, fontsize= 20)
  plt.ylabel(ylabel, fontsize= 20)

def normalise(y):
  if type(y[0]) == np.float64:
    y = (y-np.min(y))/max(1,(np.max(y)-np.min(y)))
    return y,np.min(y),np.max(y)
  else:
    min1 = []
    max1 = []
    for i in range(len(y[0])):
      min1.append(np.min(y[:,i]))
      max1.append(np.max(y[:,i]))
      if not (y[:,i]==np.min(y[:,i])).all():
        y[:,i] = (y[:,i]-np.min(y[:,i]))/max(1,(np.max(y[:,i])-np.min(y[:,i])))
    return y,min1,max1

#x = data[0]
#y = data[1]
#x1=np.array([[1,x[0],(x[0]**2)]])
#for i in range(1,len(x)):
#  x1 = np.append(x1,[[1,x[i],(x[i]**2)]],axis=0)
#x1,x_min,x_max = normalise(x1)
#x_min[0] = 0
#
#plt.figure()
#plt.plot(x,y,'o', label="true")
#plt.xlabel("time", fontsize= 20)
#plt.ylabel("height", fontsize= 20)

def hypothesis(x,w):
  return np.matmul(x,w)

class nlr:
  # Evaluates the gradient of cost function (J). Hint: You can use this to optimize w
  
  def grad(self,x,y,w):
    features = len(w)
    num_pts = len(x)
    grad_J = []
    hyp = hypothesis(x,w)
    grad_J = np.matmul(np.transpose(hyp-y),x)/y.shape[0]
    # for j in range(features):
    #   J_j=0
    #   for i in range(num_pts):
    #     J_j+=(hyp[i]-y[i])*x[i][j]
    #   grad_J.append(J_j)
    return grad_J

  # This function calculates the cost (J)
  def computeCost(self,x,y,w):
    hyp = hypothesis(x,w)
    J = np.dot(hyp-y,hyp-y)/(2*y.shape[0])
    # for i in range(num_pts):
    #   J+=(((hyp[i]-y[i])**2)/2)
    # J = J/y.shape[0]
    return J
  
  def BgradientDescent(self, x, y, w, alpha, iters):
    w = w.copy() # To keep a copy of original weights   
    J_history = []   # Use a python list to save cost in every iteration
    for i in range(iters):
      gradient = self.grad(x,y,w)
      w = w - alpha*gradient
      J_history.append(self.computeCost(x,y,w))
      # Loop to update weights (w vector)
      # Also save cost at every step

    return w, J_history

  def SgradientDescent(self, x, y, w, alpha, iters):
    w = w.copy() # To keep a copy of original weights
    J_history_s = []  
    for i in range(iters):
      r = a = np.random.randint(69)
      gradient = self.grad(x1[a:a+1],y[a:a+1],w)
      w = w - alpha*gradient
      J_history_s.append(self.computeCost(x,y,w))

    return w, J_history_s


    # def ls_secant(self,x,y,w,d):
    # # d is search direction d = -grad(J). Refer class and Lab notes
    # epsilon = 10**(-4) # Line search tolerance
    
    # alpha_curr = 0     # Alpha (x_i-1)
    # alpha = 0.01       # initial value (x_i)

    # dphi_zero =            # dphi_zero = (d^T)(grad J(w_0) # At every alpha updation loop you will have a given initial weight vector (w_0)
    # dphi_curr = dphi_zero  # required for first alpha iteration
    # i = 0
    # while abs(dphi_curr) > (epsilon*abs(dphi_zero)):  # tolerance or looping criteria used here
    #   # write loop to update alpha


    # return alpha

#w = np.array([1,1,1])
#
#hypothesis(x1,w)
#
#n = nlr()
#w_batch,J_batch = n.BgradientDescent(x1,y,w,0.01,50000)
#w_batch
#
#abs((y-hypothesis(x1,w_batch))/y)*100
#
#w_stc,J_stc = n.SgradientDescent(x1,y,w,0.01,50000)
#w_stc
#
#abs((y-hypothesis(x1,w_stc))/y)*100
#
#iterations = np.arange(1,len(J_batch)+1)
#
#plot_ex_data(iterations,J_batch,"iteration","Cost Function(BGD)")
#
#plot_ex_data(iterations,J_stc,"iteration","Cost Function(SGD)")
#
#"""alpha = 0.1"""
#
#w1,J1 = n.BgradientDescent(x1,y,w,0.1,50000)
#plot_ex_data(iterations,J1,"iteration","Cost Function(alpha=0.1)")
#
#w1,J1 = n.BgradientDescent(x1,y,w,0.5,50000)
#plot_ex_data(iterations,J1,"iteration","Cost Function(alpha=0.5)")
#
#w1,J1 = n.BgradientDescent(x1,y,w,0.05,50000)
#plot_ex_data(iterations,J1,"iteration","Cost Function(alpha=0.05)")
#
#w1,J1 = n.BgradientDescent(x1,y,w,0.01,50000)
#plot_ex_data(iterations,J1,"iteration","Cost Function(alpha=0.01)")
#
#"""As we increase alpha the dip in the curve increases i.e. it reaches toward min faster."""
#
#t = np.linspace(0.0, 2.5, num=100)
#
#input=np.array([[1,t[0],(t[0]**2)]])
#for i in range(1,len(t)):
#  input = np.append(input,[[1,t[i],(t[i]**2)]],axis=0)
#
#input = (input - np.array(x_min)) /(np.array(x_max)-np.array(x_min))
#
#output = np.matmul(input,w_batch)
#
#plt.figure()
#plt.plot(t,output,'o',label = "predicted")
#plt.plot(x,y,'o', label="true")
#plt.xlabel("time", fontsize= 20)
#plt.ylabel("height", fontsize= 20)
#plt.legend()
