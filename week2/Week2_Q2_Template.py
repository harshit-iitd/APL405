# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12fxhyJzV1tcieCrG-6PFumU1ivMMETa9
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

"""Reading Data from pandas and removing id"""

#data = pd.read_csv("prob2data.csv")
#del data['id']
#
#y = data['price'].to_numpy()
#
#data = data.assign(const = [1 for i in range(len(data))])
#data
#
#x = data[["const","bedrooms",	"bathrooms",	"sqft_living",	"floors",	"yr_built"]].to_numpy()

def plot_ex_data(x,y,xlabel,ylabel):
  plt.figure()
  plt.plot(x,y,'o')
  plt.xlabel(xlabel, fontsize= 20)
  plt.ylabel(ylabel, fontsize= 20)

"""Mean Normalisation Function"""

def normalise(y):
  if type(y[0]) == np.float64:
    mean = np.mean(y)
    sigma = np.std(y)
    min1 = np.min(y)
    max1 = np.max(y)
    y = (y-np.mean(y))/max(1,(np.max(y)-np.min(y)))
    return y,mean,min1,max1
  else:
    mean1 = []
    sigma = []
    min1 = []
    max1 = []
    for i in range(len(y[0])):
      mean1.append(np.mean(y[:,i]))
      sigma.append(np.std(y[:,i]))
      min1.append(np.min(y[:,i]))
      max1.append(np.max(y[:,i]))
      if not (y[:,i]==np.mean(y[:,i])).all():
        y[:,i] = (y[:,i]-np.mean(y[:,i]))/max(1,(np.max(y[:,i])-np.min(y[:,i])))
    return y,np.array(mean1),np.array(min1),np.array(max1)

global x_mean
global x_min
global x_max

#x,x_mean,x_min,x_max = normalise(x)

"""Model Implimentation"""

def hypothesis(x,w):
  return np.matmul(x,w)

class mr:
  # Evaluates the gradient of cost function (J). Hint: You can use this to optimize w
  def grad(self,x,y,w):
    features = len(w)
    num_pts = len(x)
    grad_J = []
    hyp = hypothesis(x,w)
    grad_J = np.matmul(np.transpose(hyp-y),x)/y.shape[0]
    # for j in range(features):
    #   J_j=0
    #   for i in range(num_pts):
    #     J_j+=(hyp[i]-y[i])*x[i][j]
    #   grad_J.append(J_j)
    return grad_J

  # This function calculates the cost (J)
  def computeCost(self,x,y,w):
    hyp = hypothesis(x,w)
    J = np.dot(hyp-y,hyp-y)/(2*y.shape[0])
    # for i in range(num_pts):
    #   J+=(((hyp[i]-y[i])**2)/2)
    # J = J/y.shape[0]
    return J
  
  # This function optimizes the weights w_0, w_1, w_2. Batch Gradient Descent method
  def bgdMulti(self, x, y, w, alpha, iters):
    w = w.copy() # To keep a copy of original weights   
    J_history = []   # Use a python list to save cost in every iteration
    for i in range(iters):
      gradient = self.grad(x,y,w)
      w = w - alpha*gradient
      J_history.append(self.computeCost(x,y,w))
      # Loop to update weights (w vector)
      # Also save cost at every step

    return w, J_history
  
  def predict(self, mu, sigma,test,weights):
    x_norm = (test - x_mean[1:])/(x_max[1:]-x_min[1:])
    x_norm = np.append([1],x_norm)
    price = np.matmul(x_norm,weights)
    return price

"""Initial weights"""

#w = np.array([0,0,0,0,0,0])
#
#"""Model Training"""
#
#m = mr()
#w_out,J_out = m.bgdMulti(x,y,w,0.01,50000)
#
#"""4 bedrooms, 2.5 bathrooms, 2570 sq. feet area, 2 floors, 2005 yr.
#built, and state the difference between the model prediction and actual value (Rs. 719000)
#"""
#
#x_test = np.array([4,2.5,2570,2,2005])
#
#"""Prediction and error"""
#
#a = m.predict(1,2,x_test,w_out)
#a
#
#(719000 - a)/719000 *100
#
#iterations = np.arange(1,len(J_out)+1)
#
#w1,J1 = m.bgdMulti(x,y,w,0.01,50000)
#w2,J2 = m.bgdMulti(x,y,w,0.05,50000)
#w3,J3 = m.bgdMulti(x,y,w,0.1,50000)
#w4,J4 = m.bgdMulti(x,y,w,0.5,50000)
#
#plot_ex_data(iterations,J1,"iteration","Cost Function(alpha=0.01)")
#
#plot_ex_data(iterations,J2,"iteration","Cost Function(alpha=0.05)")
#
#plot_ex_data(iterations,J3,"iteration","Cost Function(alpha=0.1)")
#
#plot_ex_data(iterations,J4,"iteration","Cost Function(alpha=0.5)")
#
#"""As we increase alpha the dip in the curve increases i.e. it reaches toward min faster."""
#
